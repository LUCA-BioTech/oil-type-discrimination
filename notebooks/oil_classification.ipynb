{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型：DecisionTreeClassifier\n",
      "Accuracy: 0.9166666666666666\n",
      "Recall: 0.9230769230769231\n",
      "Precision: 0.9358974358974359\n",
      "F1 Score: 0.9076923076923076\n",
      "Kappa Coefficient: 0.9130451096264045\n",
      "------\n",
      "模型：KNeighborsClassifier\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa Coefficient: 1.0\n",
      "------\n",
      "模型：GaussianNB\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa Coefficient: 1.0\n",
      "------\n",
      "模型：MLPClassifier\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa Coefficient: 1.0\n",
      "------\n",
      "模型：LinearDiscriminantAnalysis\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa Coefficient: 1.0\n",
      "------\n",
      "模型：ExtraTreesClassifier\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa Coefficient: 1.0\n",
      "------\n",
      "模型：VotingClassifier\n",
      "Accuracy: 1.0\n",
      "Recall: 1.0\n",
      "Precision: 1.0\n",
      "F1 Score: 1.0\n",
      "Kappa Coefficient: 1.0\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# import xgboost as xgb\n",
    "# import lightgbm as lgb\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef\n",
    "\n",
    "df = pd.read_csv('../data/raw/data-923.csv')\n",
    "# df = df.drop(columns=['num'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df['A_encoded'] = label_encoder.fit_transform(df['cate'])\n",
    "y = df['A_encoded']\n",
    "X = df[['en1','en2','en3','en4', 'en5', 'en6', 'en7', 'en8', 'en9', 'en10', 'en11', 'en12', 'en13', 'en14', 'en15']]\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42,stratify=y)\n",
    "\n",
    "# 定义分类器及其参数\n",
    "classifiers = {\n",
    "    'DecisionTree': DecisionTreeClassifier(criterion='entropy',random_state=42),\n",
    "    # SVM 支持多种核函数，包括线性核（linear）、多项式核（poly）、径向基函数（RBF，rbf）和sigmoid核。对于非线性问题，通常使用非线性核函数，RBF 是最常用的选择。\n",
    "    # 'SVM': SVC(probability=True, kernel='rbf',random_state=42),\n",
    "    # 'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    # 'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'KNeighbors': KNeighborsClassifier(weights='distance'),\n",
    "    'GaussianNB': GaussianNB(var_smoothing=1e-2),\n",
    "    # 'AdaBoost': AdaBoostClassifier(learning_rate=0.1,random_state=42),\n",
    "    'MLP (Neural Network)': MLPClassifier(hidden_layer_sizes=(100,), max_iter=100,learning_rate_init=0.01, random_state=42),\n",
    "    # 'Logistic Regression': LogisticRegression(),\n",
    "    'LDA': LinearDiscriminantAnalysis(),\n",
    "    # 'XGBoost': xgb.XGBClassifier(eval_metric='mlogloss'),\n",
    "    # 'LightGBM': lgb.LGBMClassifier(),\n",
    "    # 'CatBoost': CatBoostClassifier(silent=True),\n",
    "    'Extra Trees': ExtraTreesClassifier()\n",
    "}\n",
    "\n",
    "# 封装训练和评估模型的函数\n",
    "def train_evaluate(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    kappa = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    print(f\"模型：{clf.__class__.__name__}\")\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print(f'Kappa Coefficient: {kappa}')\n",
    "    print('------')\n",
    "    \n",
    "    # print(', '.join(y_test.astype(str)))\n",
    "    # print(y_pred)\n",
    "\n",
    "# 创建一个投票分类器\n",
    "voting_clf = VotingClassifier(estimators=[(name, clf) for name, clf in classifiers.items()], voting='soft')\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    train_evaluate(clf,X_train,y_train,X_test,y_test)\n",
    "\n",
    "train_evaluate(voting_clf,X_train,y_train,X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oil Type Discrimination - Analysis & Visualization\n",
    "\n",
    "## Purpose\n",
    "This notebook contains exploratory data analysis, algorithm comparison, and publication-ready visualizations used in the paper.\n",
    "\n",
    "## Paper Figures Generated\n",
    "| Figure | Description | Cell |\n",
    "|--------|-------------|------|\n",
    "| Fig 1 | Nature-style 2D scatter plot | [Nature-style Plot] |\n",
    "| Fig 2 | LDA Scree plot | [LDA Scree Plot] |\n",
    "| Fig 3 | Feature correlation heatmap | [Correlation Analysis] |\n",
    "| Fig 4 | ROC curves by LDA dimensions | [ROC Analysis] |\n",
    "| Fig 5 | 3D interactive visualization | [3D Visualization] |\n",
    "\n",
    "## Algorithms Tested\n",
    "- Decision Tree, KNN, GaussianNB, MLP\n",
    "- LDA, CatBoost, Extra Trees\n",
    "- SVM, Random Forest, Gradient Boosting\n",
    "\n",
    "## Input Data\n",
    "- File: `../data/data-923.csv`\n",
    "- Samples: 78 (13 oil types × 6 replicates)\n",
    "- Features: 15 enzyme absorbance values (en1-en15)\n",
    "\n",
    "## Output\n",
    "All figures are saved to the current directory in PDF/SVG format.\n",
    "\n",
    "## Note\n",
    "The core plotting functions from this notebook have been extracted to `src/visualization/` for reusability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef\nimport matplotlib.pyplot as plt\nimport os\n\n# 创建输出目录\nos.makedirs(\"output\", exist_ok=True)\n\n# ============================================================\n# Nature 风格可视化函数（右侧图例）\n# ============================================================\ndef plot_nature_scatter(X2d, y_labels, title=\"Nature-style Plot\", save_name=None):\n    plt.rcParams.update({\n        \"font.family\": \"Arial\",\n        \"figure.dpi\": 300,\n        \"savefig.dpi\": 300,\n        \"axes.linewidth\": 1.2,\n        \"axes.labelsize\": 14,\n        \"axes.titlesize\": 15,\n        \"xtick.labelsize\": 12,\n        \"ytick.labelsize\": 12,\n        \"xtick.major.width\": 1.0,\n        \"ytick.major.width\": 1.0,\n        \"legend.frameon\": False,\n        \"axes.facecolor\": \"white\",\n        \"axes.edgecolor\": \"black\",\n    })\n\n    nature_colors = [\n        \"#4C72B0\", \"#DD8452\", \"#55A868\",\n        \"#C44E52\", \"#8172B3\", \"#937860\",\n        \"#DA8BC3\", \"#8C8C8C\", \"#64B5CD\"\n    ]\n\n    plt.figure(figsize=(6.5, 5))\n    unique_classes = np.unique(y_labels)\n\n    for i, cls in enumerate(unique_classes):\n        idx = (y_labels == cls)\n        plt.scatter(\n            X2d[idx, 0], X2d[idx, 1],\n            s=55, alpha=0.78,\n            color=nature_colors[i % len(nature_colors)],\n            edgecolor=\"black\", linewidth=0.35,\n            label=str(cls)\n        )\n\n    ax = plt.gca()\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    plt.xlabel(\"Component 1\", labelpad=6)\n    plt.ylabel(\"Component 2\", labelpad=6)\n    plt.title(title, pad=10)\n\n    # 图例在右侧\n    plt.legend(\n        loc=\"center left\",\n        bbox_to_anchor=(1.02, 0.5),\n        frameon=False,\n        borderaxespad=0,\n        handletextpad=0.3\n    )\n\n    plt.tight_layout()\n    if save_name:\n        plt.savefig(f\"output/{save_name}.pdf\", bbox_inches=\"tight\")\n        plt.savefig(f\"output/{save_name}.svg\", bbox_inches=\"tight\")\n    plt.show()\n\n\n# ============================================================\n# 读取数据\n# ============================================================\ndf = pd.read_csv(\"../data/raw/data-923.csv\")\nlabel_encoder = LabelEncoder()\ndf[\"cate_str\"] = df[\"cate\"].astype(str)\ndf[\"cate_encoded\"] = label_encoder.fit_transform(df[\"cate_str\"])\n\ny = df[\"cate_encoded\"]\nX = df[[f\"en{i}\" for i in range(1, 16)]]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ============================================================\n# PCA / LDA 降维\n# ============================================================\nuse_lda = True  # True 使用 LDA，False 使用 PCA\n\nif use_lda:\n    # LDA 降维，最多 n_classes-1\n    lda = LinearDiscriminantAnalysis(n_components= min(8, len(np.unique(y))-1))\n    X_reduced = lda.fit_transform(X_scaled, y)\nelse:\n    # PCA 降维\n    pca = PCA(n_components=8)\n    X_reduced = pca.fit_transform(X_scaled)\n\n# ============================================================\n# 划分训练集（全部降维后的维度用于训练）\n# ============================================================\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reduced, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# ============================================================\n# 模型列表\n# ============================================================\nmodels = {\n    \"Decision Tree\": DecisionTreeClassifier(criterion=\"entropy\"),\n    #\"KNN\": KNeighborsClassifier(weights=\"distance\"),\n    \"Naive Bayes\": GaussianNB(var_smoothing=1e-2),\n    #\"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=150, learning_rate_init=0.01),\n    #\"CatBoost\": CatBoostClassifier(silent=True),\n    #\"Extra Trees\": ExtraTreesClassifier()\n}\n\n# ============================================================\n# 训练 + 输出指标 + 可视化（前两维）\n# ============================================================\nfor name, clf in models.items():\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    y_pred_str = label_encoder.inverse_transform(y_pred)\n\n    # 输出指标\n    print(f\"\\n模型：{name}\")\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}; \"\n          f\"Recall: {recall_score(y_test, y_pred, average='macro')}; \"\n          f\"Precision: {precision_score(y_test, y_pred, average='macro')}; \"\n          f\"F1: {f1_score(y_test, y_pred, average='macro')}; \"\n          f\"MCC(Kappa): {matthews_corrcoef(y_test, y_pred)}\")\n\n    # 可视化：只用降维后的前两维定义 XY\n    plot_nature_scatter(\n        X_reduced[:, :2],\n        label_encoder.inverse_transform(clf.predict(X_reduced)),\n        title=f\"{name} Prediction (2D Visualization)\",\n        save_name=f\"{name}_2D_Nature\"\n    )\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom scipy.stats import chi2\nimport os\n\n# ===================== 工具函数：生成置信椭球 (保持不变) =====================\ndef get_confidence_ellipsoid(x, y, z, confidence=0.9):\n    data = np.vstack((x, y, z))\n    if data.shape[1] < 4: return None, None, None\n    mean = np.mean(data, axis=1)\n    cov = np.cov(data)\n    eigvals, eigvecs = np.linalg.eigh(cov)\n    scale_factor = np.sqrt(chi2.ppf(confidence, df=3))\n    u = np.linspace(0, 2 * np.pi, 20)\n    v = np.linspace(0, np.pi, 20)\n    x_sphere = np.outer(np.cos(u), np.sin(v))\n    y_sphere = np.outer(np.sin(u), np.sin(v))\n    z_sphere = np.outer(np.ones(np.size(u)), np.cos(v))\n    sphere_points = np.stack((x_sphere.flatten(), y_sphere.flatten(), z_sphere.flatten()))\n    radii = np.sqrt(np.maximum(eigvals, 0)) * scale_factor\n    transformed = (eigvecs @ np.diag(radii) @ sphere_points).T + mean\n    return transformed[:, 0], transformed[:, 1], transformed[:, 2]\n\n# ===================== 1. 数据准备 =====================\ndf = pd.read_csv(\"../data/raw/data-923.csv\")\nlabel_encoder = LabelEncoder()\ndf[\"cate_encoded\"] = label_encoder.fit_transform(df[\"cate\"].astype(str))\ny = df[\"cate_encoded\"]\nX = df[[f\"en{i}\" for i in range(1, 16)]]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# LDA 降维\nlda = LinearDiscriminantAnalysis(n_components=min(3, len(np.unique(y)) - 1))\nX_reduced_all = lda.fit_transform(X_scaled, y)\nX_plot_3d = X_reduced_all[:, :3]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reduced_all, y, test_size=0.3, stratify=y, random_state=42\n)\n\n# ===================== 2. 模型训练与指标计算 (测试集) =====================\nmodel = GaussianNB(var_smoothing=1e-2)\nmodel.fit(X_train, y_train)\n\ny_pred_test = model.predict(X_test)\ncm = confusion_matrix(y_test, y_pred_test)\n\nmetric_map = {}\nunique_labels_encoded = np.unique(y_test)\nunique_labels_str = label_encoder.inverse_transform(unique_labels_encoded)\n\nfor i, label_str in enumerate(unique_labels_str):\n    tp = cm[i, i]\n    total_true = cm[i, :].sum()\n    recall = tp / total_true if total_true > 0 else 0\n    total_pred = cm[:, i].sum()\n    precision = tp / total_pred if total_pred > 0 else 0\n    metric_map[label_str] = f\" | R:{recall:.2f}, P:{precision:.2f}\"\n    # print(f\"Class {label_str}: Recall={recall:.2f}, Precision={precision:.2f}\") # 可以取消注释查看\n\n# --- 准备全量数据的绘图数据 ---\nall_probs = model.predict_proba(X_reduced_all)\nconfidence_scores = np.max(all_probs, axis=1)\ny_pred = model.predict(X_reduced_all)\ny_pred_str = label_encoder.inverse_transform(y_pred)\ny_true_str = label_encoder.inverse_transform(y)\n\ndf_plot = pd.DataFrame({\n    \"Comp1\": X_plot_3d[:, 0], \"Comp2\": X_plot_3d[:, 1], \"Comp3\": X_plot_3d[:, 2],\n    \"True Label\": y_true_str, \"Predicted Label\": y_pred_str, \"Confidence\": confidence_scores,\n    \"Is_Correct\": y == y_pred\n})\ndf_plot[\"Status\"] = np.where(df_plot[\"Is_Correct\"], \"Correct\", \"Misclassified\")\n\n# ===================== 3. 绘图：颜色和图例优化 =====================\noutput_dir = \"output/3D_Optimized_Legend\"\nos.makedirs(output_dir, exist_ok=True)\n\n# --- 关键修改：生成足够多的不重复颜色 ---\n# 如果类别很多，可能需要 Plotly 之外的调色板，例如 matplotlib 的'tab20'或'Paired'\n# 这里使用 Plotly 默认的定性色板，它通常能处理10个左右的不同颜色\n# 如果类别数大于len(px.colors.qualitative.Plotly)，颜色会重复。\n# 更好的做法是生成一个更长的颜色列表，例如：\nnum_classes = len(unique_labels_str)\ncolors_base = px.colors.qualitative.Alphabet if num_classes > 10 else px.colors.qualitative.Plotly\ncolors_extended = colors_base * ((num_classes // len(colors_base)) + 1)\ncolor_map = {label: colors_extended[i] for i, label in enumerate(unique_labels_str)}\n\n\nfig = go.Figure()\nprint(\"正在生成图表...\")\n\n# 添加一个额外的空散点图，专门用于图例中的 \"Misclassified\" 标记\n# 这个标记将是统一的，不区分具体类别，仅代表\"预测错误\"这一概念\nfig.add_trace(go.Scatter3d(\n    x=[None], y=[None], z=[None], # 不显示实际点\n    mode='markers',\n    marker=dict(symbol='diamond-open', color='rgba(0,0,0,0.5)', size=8), # 统一的灰色菱形\n    name='Misclassified Points', # 图例名称\n    showlegend=True,\n    legendgroup='_misc', # 单独分组\n    hoverinfo='skip'\n))\n\n\nfor label in unique_labels_str:\n    subset = df_plot[df_plot[\"True Label\"] == label]\n    color = color_map[label]\n    metrics_suffix = metric_map.get(label, \"\")\n    \n    # --- A. 绘制 90% 置信椭球 (主图例项，带有颜色色块和指标) ---\n    ex, ey, ez = get_confidence_ellipsoid(\n        subset[\"Comp1\"].values, subset[\"Comp2\"].values, subset[\"Comp3\"].values, confidence=0.9\n    )\n    if ex is not None:\n        fig.add_trace(go.Mesh3d(\n            x=ex, y=ey, z=ez,\n            alphahull=0, opacity=0.15, color=color,\n            name=f\"{label}{metrics_suffix}\", # 显示类别和指标\n            legendgroup=label,\n            showlegend=False,\n            hoverinfo='skip'\n        ))\n        # 更好的方法是添加一个不可见的散点图来控制图例标记\n        fig.add_trace(go.Scatter3d(\n            x=[None], y=[None], z=[None], # 不显示点\n            mode='markers',\n            marker=dict(color=color, symbol='circle', size=10), # 圆形色块\n            name=f\"{label}{metrics_suffix}\", # 再次命名，这个trace是用来显示图例的\n            legendgroup=label,\n            showlegend=True, # 显示图例\n            hoverinfo='skip',\n            visible='legendonly', # 默认不显示在图上，只显示在图例中\n            legendrank=0 # 确保它排在最前面\n        ))\n\n\n    # --- B. 绘制散点 (区分正确与错误) ---\n    # 正确分类的散点，不显示图例，只显示在图上\n    sub_subset_correct = subset[subset[\"Status\"] == \"Correct\"]\n    if not sub_subset_correct.empty:\n        custom_data_correct = np.stack((\n            sub_subset_correct[\"True Label\"], sub_subset_correct[\"Predicted Label\"],\n            sub_subset_correct[\"Status\"], sub_subset_correct[\"Confidence\"]\n        ), axis=-1)\n        fig.add_trace(go.Scatter3d(\n            x=sub_subset_correct[\"Comp1\"], y=sub_subset_correct[\"Comp2\"], z=sub_subset_correct[\"Comp3\"],\n            mode='markers',\n            name=f\"{label} (Correct)\", # 这个name会显示在hover中\n            legendgroup=label,\n            showlegend=False, # 不在图例中显示\n            marker=dict(\n                size=sub_subset_correct[\"Confidence\"] * 10 + 2,\n                color=color,\n                symbol='circle',\n                opacity=0.9,\n                line=dict(width=0)\n            ),\n            customdata=custom_data_correct,\n            hovertemplate=(\n                \"<b>True Label: %{customdata[0]}</b><br>\" +\n                \"Predicted: %{customdata[1]}<br>\" +\n                \"Status: %{customdata[2]}<br>\" +\n                \"Confidence: %{customdata[3]:.1%}<br>\" +\n                \"<extra></extra>\"\n            )\n        ))\n    \n    # 错误分类的散点，现在是统一的灰色菱形，会有一个单独的图例项\n    sub_subset_misclassified = subset[subset[\"Status\"] == \"Misclassified\"]\n    if not sub_subset_misclassified.empty:\n        custom_data_misclassified = np.stack((\n            sub_subset_misclassified[\"True Label\"], sub_subset_misclassified[\"Predicted Label\"],\n            sub_subset_misclassified[\"Status\"], sub_subset_misclassified[\"Confidence\"]\n        ), axis=-1)\n        fig.add_trace(go.Scatter3d(\n            x=sub_subset_misclassified[\"Comp1\"], y=sub_subset_misclassified[\"Comp2\"], z=sub_subset_misclassified[\"Comp3\"],\n            mode='markers',\n            name=f\"{label} (Misclassified)\", # 仍然用于hover\n            legendgroup='_misc', # 分配到统一的错误分类组\n            showlegend=False, # 不显示在图例中，因为我们已经有统一的 \"Misclassified Points\"\n            marker=dict(\n                size=sub_subset_misclassified[\"Confidence\"] * 10 + 2,\n                color=color, # 颜色仍然是其True Label的颜色\n                symbol='diamond-open', # 空心菱形\n                opacity=0.9,\n                line=dict(width=0)\n            ),\n            customdata=custom_data_misclassified,\n            hovertemplate=(\n                \"<b>True Label: %{customdata[0]}</b><br>\" +\n                \"Predicted: %{customdata[1]}<br>\" +\n                \"Status: %{customdata[2]}<br>\" +\n                \"Confidence: %{customdata[3]:.1%}<br>\" +\n                \"<extra></extra>\"\n            )\n        ))\n\n# ===================== 4. 布局优化 =====================\nfig.update_layout(\n    title=\"3D Analysis: Per-Class Test Set Metrics in Legend (Optimized)\",\n    scene=dict(\n        xaxis_title='LDA Component 1',\n        yaxis_title='LDA Component 2',\n        zaxis_title='LDA Component 3',\n        aspectmode='cube'\n    ),\n    width=1200,\n    height=900,\n    legend=dict(\n        title=\"Class (R=Recall, P=Precision)\",\n        itemsizing='constant',\n        groupclick=\"toggleitem\"\n    )\n)\n\nhtml_file = os.path.join(output_dir, \"Optimized_Legend_Analysis.html\")\nfig.write_html(html_file)\nprint(f\"Saved: {html_file}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "# ===================== 自定义颜色 =====================\n",
    "custom_colors = [\n",
    "    \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\",\n",
    "    \"#8c564b\", \"#e377c2\", \"#7f7f7f\", \"#bcbd22\", \"#17becf\",\n",
    "    \"#aec7e8\", \"#ffbb78\", \"#98df8a\"\n",
    "]\n",
    "\n",
    "# ===================== 数据读取与预处理 =====================\n",
    "df = pd.read_csv(\"../data/raw/data-923.csv\")\n",
    "cols = [f\"en{i}\" for i in range(1,16)]\n",
    "X = df[cols].values\n",
    "y_raw = df[\"cate\"].astype(str).values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "class_names = le.classes_\n",
    "n_classes = len(class_names)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_y_true, all_y_pred, all_y_prob = [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_scaled, y):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_classes-1)\n",
    "    X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "    X_test_lda = lda.transform(X_test)\n",
    "\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train_lda, y_train)\n",
    "    y_pred = model.predict(X_test_lda)\n",
    "    y_prob = model.predict_proba(X_test_lda)\n",
    "\n",
    "    all_y_true.append(y_test)\n",
    "    all_y_pred.append(y_pred)\n",
    "    all_y_prob.append(y_prob)\n",
    "\n",
    "y_true_all = np.concatenate(all_y_true)\n",
    "y_pred_all = np.concatenate(all_y_pred)\n",
    "y_prob_all = np.concatenate(all_y_prob)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# 确保输出目录存在\nos.makedirs(\"output\", exist_ok=True)\n\n# 特征数据\ncols = [f\"en{i}\" for i in range(1,16)]\nX_df = df[cols]\n\n# 相关系数矩阵\ncorr_matrix = X_df.corr()\n\n# 创建一个 mask，只显示上三角的颜色\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n\nfig, ax = plt.subplots(figsize=(10,8))\n\n# 上三角热力图（颜色）\nsns.heatmap(\n    corr_matrix,\n    mask=~mask,            # 上三角\n    annot=False,           # 不显示数字\n    cmap=\"coolwarm\",\n    vmin=-1, vmax=1,\n    cbar=True,\n    linewidths=0.3,\n    linecolor='gray',\n    square=True,\n    ax=ax\n)\n\n# 下三角数字显示\nfor i in range(corr_matrix.shape[0]):\n    for j in range(i):\n        ax.text(j+0.5, i+0.5,\n                f\"{corr_matrix.iloc[i,j]:.2f}\",\n                ha='center', va='center', fontsize=9, color='black')\n\n# 坐标轴和标题\nax.set_xticks(np.arange(corr_matrix.shape[0])+0.5)\nax.set_yticks(np.arange(corr_matrix.shape[0])+0.5)\nax.set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=10)\nax.set_yticklabels(corr_matrix.columns, rotation=0, fontsize=10)\nax.set_title(\"Feature Correlation Matrix\", fontsize=13)\n\n# 极细边框\nfor spine in ax.spines.values():\n    spine.set_linewidth(0.3)\n\nax.grid(\n         linestyle='-', \n         alpha=0.7, \n         color='lightgray', \n         linewidth=0.3)\n\nplt.tight_layout()\nplt.savefig(\"output/Feature_Correlation_Matrix_Professional.pdf\", dpi=300)\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n# 确保输出目录存在\nos.makedirs(\"output\", exist_ok=True)\n\n# 假设 explained_var 和 cum_var 已经由 LDA fit 得到\nexplained_var = lda.explained_variance_ratio_\ncum_var = np.cumsum(explained_var)\nld_indices = np.arange(1, len(explained_var)+1)\n\nfig, ax1 = plt.subplots(figsize=(6,4))\n\n# ------------------- 左轴：柱状图 -------------------\nbars = ax1.bar(ld_indices, explained_var, color=\"#1f77b4\", alpha=0.85,\n               edgecolor='k', linewidth=0, label=\"Individual LD\")\nax1.set_xlabel(\"LDA Component\", fontsize=10)\nax1.set_ylabel(\"Explained Variance Ratio\", fontsize=10, color=\"#1f77b4\")\nax1.tick_params(axis='y', labelcolor=\"#1f77b4\", labelsize=9, width=0.5, length=4)\nax1.tick_params(axis='x', width=0.5, length=4)\nax1.set_xticks(ld_indices)\nax1.set_ylim(0, max(explained_var)*1.3)\n\n# 柱顶标注每个 LD 的贡献率\nfor bar in bars:\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., 1.02*height,\n             f'{height:.2f}', ha='center', va='bottom', fontsize=7)\n\n# ------------------- 右轴：累计折线 -------------------\nax2 = ax1.twinx()\nax2.plot(ld_indices, cum_var*100, color=\"#ff7f0e\", marker='o', markersize=5,\n         linewidth=1.8, label=\"Cumulative Explained\")\nax2.set_ylabel(\"Cumulative Explained Variance (%)\", fontsize=10, color=\"#ff7f0e\")\nax2.tick_params(axis='y', labelcolor=\"#ff7f0e\", labelsize=9, width=0.5, length=4)\nax2.set_ylim(0, 105)\n\n# 标注达到80%累计解释率的LD\nthreshold = 80\nfirst_above = np.where(cum_var*100 >= threshold)[0][0] + 1\nax2.axvline(x=first_above, color='grey', linestyle='--', linewidth=0.5)\nax2.text(first_above+0.3, 30, f'{threshold}% variance at LD{first_above}',\n         color='grey', fontsize=8)\n\n# ------------------- 坐标轴极细 -------------------\nfor spine in ax1.spines.values():\n    spine.set_linewidth(0.2)\nfor spine in ax2.spines.values():\n    spine.set_linewidth(0.2)\n\nax1.grid(\n         linestyle='-', \n         alpha=0.7, \n         color='lightgray', \n         linewidth=0.3)\nax2.grid(\n         linestyle='-', \n         alpha=0.7, \n         color='lightgray', \n         linewidth=0.3)\n# ------------------- 网格和美化 -------------------\nfig.suptitle(\"LDA Scree Plot\", fontsize=12)\nfig.tight_layout()\n\n# ------------------- 高精度输出 -------------------\nplt.savefig(\"output/Fig2a_LDA_scree_Nature_Final.pdf\", dpi=300)\nplt.show()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport os\n\n# 确保输出目录存在\nos.makedirs(\"output\", exist_ok=True)\n\n# ------------------- 保存 LDA Scree 数据到 CSV -------------------\ndf_lda_scree = pd.DataFrame({\n    \"LD\": ld_indices,\n    \"explained_variance_ratio\": explained_var,\n    \"cumulative_explained_variance\": cum_var,\n    \"cumulative_explained_variance_%\": cum_var * 100\n})\n\ndf_lda_scree.to_csv(\"output/Fig2a_LDA_scree_data.csv\", index=False)\nprint(\"Saved: output/Fig2a_LDA_scree_data.csv\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve, auc\nimport os\n\n# 确保输出目录存在\nos.makedirs(\"output\", exist_ok=True)\n\n# ===================== 数据读取与预处理 =====================\ndf = pd.read_csv(\"../data/raw/data-923.csv\")\ncols = [f\"en{i}\" for i in range(1,16)]\nX = df[cols].values\ny_raw = df[\"cate\"].astype(str).values\nle = LabelEncoder()\ny = le.fit_transform(y_raw)\nclass_names = le.classes_\nn_classes = len(class_names) # 类别数\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ===================== 遍历维度与性能评估 =====================\n\n# LDA 最大维度为 n_classes - 1 (这里是 13 - 1 = 12)\nmax_lda_dim = n_classes - 1 \n\n# 存储每种维度下的平均 ROC 数据\nresults_by_dim = {}\n\n# 使用 Plotly 的颜色，确保不同维度有区分\ncolors_extended = px.colors.qualitative.Plotly * 2 \n\n# 遍历所有可能的 LDA 维度\nfor n_dim in range(1, max_lda_dim + 1):\n    \n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    y_true_all_cv, y_prob_all_cv = [], []\n\n    # 进行 5 折交叉验证\n    for train_idx, test_idx in kf.split(X_scaled, y):\n        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n        y_train, y_test = y[train_idx], y[test_idx]\n        \n        # 1. LDA 降维\n        lda = LinearDiscriminantAnalysis(n_components=n_dim)\n        X_train_lda = lda.fit_transform(X_train, y_train)\n        X_test_lda = lda.transform(X_test)\n\n        # 2. GaussianNB 模型训练与预测\n        model = GaussianNB()\n        model.fit(X_train_lda, y_train)\n        \n        # 收集真实标签和预测概率\n        y_prob = model.predict_proba(X_test_lda)\n        y_true_all_cv.append(y_test)\n        y_prob_all_cv.append(y_prob)\n\n    # 3. 汇总 5 折 CV 结果\n    y_true_all = np.concatenate(y_true_all_cv)\n    y_prob_all = np.concatenate(y_prob_all_cv)\n\n    # 4. 计算整体的 AUC 和 ROC 曲线\n    y_true_bin = label_binarize(y_true_all, classes=np.arange(n_classes))\n    \n    # 将多分类问题视为 One-vs-All，并压平成一维进行整体 ROC 计算\n    fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_prob_all.ravel())\n    roc_auc = auc(fpr, tpr)\n\n    # 存储结果\n    results_by_dim[n_dim] = {\n        'fpr': fpr,\n        'tpr': tpr,\n        'auc': roc_auc,\n        'color': colors_extended[n_dim - 1] # 确保每条曲线颜色不同\n    }\n\n# ===================== 绘图：对比所有维度的 ROC 曲线 =====================\nfig, ax = plt.subplots(figsize=(8,6))\nax.plot([0,1], [0,1], '--', color='grey', lw=0.8)\n\n# 绘制每种维度的 ROC 曲线\nbest_auc = -1\nbest_dim = 0\n\nfor n_dim, result in results_by_dim.items():\n    label = f\"LDA Dim: {n_dim} (AUC={result['auc']:.3f})\"\n    ax.plot(result['fpr'], result['tpr'], \n            color=result['color'], \n            lw=1.5, \n            label=label,\n            alpha=0.7)\n    \n    # 记录最佳性能的维度\n    if result['auc'] > best_auc:\n        best_auc = result['auc']\n        best_dim = n_dim\n\n# 坐标轴、标题、网格\nax.set_xlabel(\"False Positive Rate\", fontsize=12)\nax.set_ylabel(\"True Positive Rate\", fontsize=12)\nax.set_title(f\"ROC Curve Comparison (5-Fold CV) - Best Dim: {best_dim} (AUC={best_auc:.3f})\", fontsize=14)\nax.tick_params(axis='both', labelsize=11, width=0.5, length=4)\n\n# 极细坐标轴\nfor spine in ax.spines.values():\n    spine.set_linewidth(0.4)\n\n# 调整图例位置，避免遮挡曲线\nax.legend(fontsize=9, loc=\"lower right\", frameon=True, framealpha=0.95, ncols=2)\n\nax.grid(linestyle='--', alpha=0.5, color='lightgray', linewidth=0.3)\n\nplt.tight_layout()\nplt.savefig(\"output/Fig2b_ROC_LDA_Dimension_Comparison.pdf\", dpi=300)\nplt.show()\n\nprint(f\"\\n最佳 LDA 维度为: {best_dim}，对应的平均 AUC 值为: {best_auc:.3f}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport os\n\n# 确保输出目录存在\nos.makedirs(\"output\", exist_ok=True)\n\n# ===================== 保存 AUC 汇总结果 =====================\nauc_summary = pd.DataFrame([\n    {\"lda_dim\": n_dim, \"auc\": result[\"auc\"]}\n    for n_dim, result in results_by_dim.items()\n])\n\nauc_summary.to_csv(\n    \"output/Fig2b_LDA_Dimension_AUC_Summary.csv\",\n    index=False\n)\nprint(\"Saved: output/Fig2b_LDA_Dimension_AUC_Summary.csv\")\n\n# ===================== 保存 ROC 曲线数据 =====================\nroc_records = []\n\nfor n_dim, result in results_by_dim.items():\n    fpr = result[\"fpr\"]\n    tpr = result[\"tpr\"]\n    \n    for i in range(len(fpr)):\n        roc_records.append({\n            \"lda_dim\": n_dim,\n            \"fpr\": fpr[i],\n            \"tpr\": tpr[i]\n        })\n\ndf_roc = pd.DataFrame(roc_records)\n\ndf_roc.to_csv(\n    \"output/Fig2b_LDA_Dimension_ROC_Curves.csv\",\n    index=False\n)\nprint(\"Saved: output/Fig2b_LDA_Dimension_ROC_Curves.csv\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport os\n\n# 确保输出目录存在\nos.makedirs(\"output\", exist_ok=True)\n\n# ===================== 获取最后一折测试集结果 =====================\n# X_test, y_test, y_pred 是循环中最后一折的值\n# 如果你希望展示特定折，可改成对应索引\ncm = confusion_matrix(y_test, y_pred)\nn_classes = cm.shape[0]\n\n# ===================== 绘图 =====================\nfig, ax = plt.subplots(figsize=(8,6))\nax.set_facecolor(\"white\")\ndiag_color = \"#ff7f0e\"  # 对角线颜色\n\n# 绘制对角线矩形\nfor i in range(n_classes):\n    ax.add_patch(plt.Rectangle((i, i), 1, 1, fill=True, color=diag_color, alpha=0.8))\n\n# 添加绝对值注释\nfor i in range(n_classes):\n    for j in range(n_classes):\n        ax.text(j+0.5, i+0.5, str(cm[i,j]),\n                ha='center', va='center', fontsize=10, color='black')\n\n# 设置刻度和标签\nax.set_xticks(np.arange(n_classes)+0.5)\nax.set_yticks(np.arange(n_classes)+0.5)\nax.set_xticklabels(class_names, ha='right', fontsize=10)\nax.set_yticklabels(class_names, fontsize=10)\n\n# 坐标轴和标题\nax.set_xlabel(\"Predicted\", fontsize=11)\nax.set_ylabel(\"True\", fontsize=11)\nax.set_title(\"Confusion Matrix — Last Fold Test Set\", fontsize=13)\n\n# 隐藏边框\nfor spine in ax.spines.values():\n    spine.set_visible(False)\n\nax.set_xlim(0, n_classes)\nax.set_ylim(0, n_classes)\nax.invert_yaxis()\nax.set_aspect('equal')\n\nax.grid(\n         linestyle='--', \n         alpha=0.7, \n         color='lightgray', \n         linewidth=0.3)\n\nplt.tight_layout()\nplt.savefig(\"output/ConfusionMatrix_TestSet.pdf\", dpi=300)\nplt.show()\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oil_3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}